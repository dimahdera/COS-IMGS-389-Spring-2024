{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "Coding a Decision Tree .ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimahdera/ELEE-6399-ELEE-4333-AI-course/blob/main/Coding_a_Decision_Tree_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4nsk5qrnVPp"
      },
      "source": [
        "# Importing required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wNgIsgsnVPr",
        "outputId": "42f81d91-2d9d-4633-f63e-7a76437cdd1c"
      },
      "source": [
        "# The scikit-learn dataset library already has the iris dataset. \n",
        "# You can either use the dataset from the source or import it from the scikit-learn dataset library.\n",
        "\n",
        "#Loading the iris data\n",
        "data = load_iris()\n",
        "print('Classes to predict: ', data.target_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classes to predict:  ['setosa' 'versicolor' 'virginica']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NyK4BmCnVPt",
        "outputId": "1e6e521c-77c4-46a6-a8a5-dda78320eb2a"
      },
      "source": [
        "# There are three classes of iris plants: 'setosa', 'versicolor' and 'virginica'. \n",
        "# Now, we have imported the iris data in the variable 'data'. \n",
        "# We will now extract the attribute data and the corresponding labels. \n",
        "# We can extract the attributes and labels by calling .data and .target as shown below:\n",
        "\n",
        "# Extracting data attributes\n",
        "X = data.data\n",
        "### Extracting target/ class labels\n",
        "y = data.target\n",
        "\n",
        "print('Number of examples in the data:', X.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of examples in the data: 150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eH5yC0uLnVPt",
        "outputId": "08474dbd-c93c-401b-c50e-d8db5e5741d3"
      },
      "source": [
        "# There are 150 examples/ samples in the data. \n",
        "# The variable 'X' contains the attributes to the iris plant. \n",
        "# The cell below shows the 4 attributes of the first four iris plants.\n",
        "\n",
        "\n",
        "# First four rows in the variable 'X'\n",
        "X[:4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [4.6, 3.1, 1.5, 0.2]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "140OjkYWnVPu"
      },
      "source": [
        "# Now that we have extracted the data attributes and corresponding labels, \n",
        "# we will split them to form train and test datasets. \n",
        "# For this purpose, we will use the scikit-learn's 'train_test_split' function, \n",
        "# which takes in the attributes and labels as inputs and produces the train and test sets.\n",
        "\n",
        "\n",
        "# Using the train_test_split to create train and test sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 47, test_size = 0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds2q1HNnnVPu"
      },
      "source": [
        "# Since, this is a classification problem, we will import the DecisionTreeClassifier function from the sklearn library. \n",
        "# Next, we will set the 'criterion' to 'entropy', which sets the measure for splitting the attribute to information gain.\n",
        "\n",
        "# Importing the Decision tree classifier from the sklearn library.\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf = DecisionTreeClassifier(criterion = 'entropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCi1pJNfnVPv",
        "outputId": "e0682394-94e6-4a4d-eabd-1f12a0c697e7"
      },
      "source": [
        "# Next, we will fit the classifier on the train attributes and labels.\n",
        "\n",
        "# Training the decision tree classifier. \n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
              "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNpdpHVdnVPv"
      },
      "source": [
        "# Now, we will use the trained classifier/ model to predict the labels of the test attributes.\n",
        "\n",
        "# Predicting labels on the test set.\n",
        "y_pred =  clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhjAmNK5nVPv",
        "outputId": "2618a435-9d1a-4a06-d6b8-a071dbbb7697"
      },
      "source": [
        "# We will now evaluate the predicted classes using some metrics. \n",
        "# For this case, we will use 'accuracy_score' to calculate the accuracy of the predicted labels.\n",
        "\n",
        "# Importing the accuracy metric from sklearn.metrics library\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "print('Accuracy Score on train data: ', accuracy_score(y_true=y_train, y_pred=clf.predict(X_train)))\n",
        "print('Accuracy Score on test data: ', accuracy_score(y_true=y_test, y_pred=y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score on train data:  1.0\n",
            "Accuracy Score on test data:  0.9736842105263158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTsSjXCvnVPw",
        "outputId": "b308fda4-77ea-4b90-d397-d3d257752523"
      },
      "source": [
        "# Next, we will tune the parameters of the decision tree to increase its accuracy. \n",
        "# One of those parameters is 'min_samples_split', which is the minimum number of samples required to split an internal node.\n",
        "# Its default value is equal to 2 because we cannot split on a node containing only one example/ sample.\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='entropy', min_samples_split=50)\n",
        "clf.fit(X_train, y_train)\n",
        "print('Accuracy Score on train data: ', accuracy_score(y_true=y_train, y_pred=clf.predict(X_train)))\n",
        "print('Accuracy Score on the test data: ', accuracy_score(y_true=y_test, y_pred=clf.predict(X_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Score on train data:  0.9553571428571429\n",
            "Accuracy Score on the test data:  0.9736842105263158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApuuNwbPnVPx"
      },
      "source": [
        "# We can see that the accuracy on the test set increased, while it decreased on the training set. \n",
        "# This is because increasing the value of the min_sample_split smoothens the decision boundary and thus prevents it \n",
        "# from overfitting. \n",
        "# We may tune other parameters of the decision tree and check how they affect the decision boundary in a similar way. \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KI3COBBnVPx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7AMmLztnVPx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}